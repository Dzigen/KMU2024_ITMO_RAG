{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/home/dzigen/Desktop/ITMO/ВКР/КМУ2024/inference.ipynb\")\n",
    "\n",
    "from src.retrievers.bm25colbert import BM25ColBertRetriever\n",
    "from src.retrievers.bm25e5 import BM25E5Retriever\n",
    "from src.readers.fid import FiDReader\n",
    "from src.retrievers.e5 import E5Retriever\n",
    "\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNED_READER_PATH = '/home/dzigen/Desktop/ITMO/ВКР/КМУ2024/logs/join_e5_fid_triviaqa/reader_bestmodel.pt'\n",
    "BASE_PATH = '/home/dzigen/Desktop/ITMO/ВКР/КМУ2024/data/bases/e5_scipdf_base'\n",
    "READER_INPUT_FORMAT = \"context: {c}\\n\\nquestion: {q}\"\n",
    "READER_GEN_ML = 256\n",
    "\n",
    "QUESTIONS = [\n",
    "    \"What is a RETRO approach\",\n",
    "    \"What is a kNN-LM approach\",\n",
    "    \"What is a DPR approach\",\n",
    "    \"What is a RAG approach\",\n",
    "    \"What is a FiD approach\",\n",
    "    \"What is a EMDR2 approach\",\n",
    "    \"What is a Atlas approach\",\n",
    "    \"What is a REPLUG approach\",\n",
    "    \"What is a ColBERT approach\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(reader, retriever, queries):\n",
    "    answers = []\n",
    "    for query in tqdm(queries):\n",
    "        print(\"QUERY: \", query)\n",
    "        texts, k_scores, metadata = retriever.search(query)\n",
    "\n",
    "        print(\"CONTEXTS:\\n\", '\\n\\n'.join(texts))\n",
    "\n",
    "        formated_txts = list(map(\n",
    "            lambda t: READER_INPUT_FORMAT.format(q=query,c=t), texts))\n",
    "\n",
    "        tokenized_txts = reader.tokenize(formated_txts)\n",
    "        \n",
    "        cands_k = len(texts)\n",
    "\n",
    "        # Generating Answers by predicted indices\n",
    "        output = reader.model.generate(\n",
    "            input_ids=tokenized_txts['input_ids'].view(1, cands_k, -1),\n",
    "            attention_mask=tokenized_txts['attention_mask'].view(1, cands_k, -1), \n",
    "            max_length=READER_GEN_ML, eos_token_id=reader.tokenizer.eos_token_id)\n",
    "        \n",
    "        predicted = reader.tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "        answers += predicted\n",
    "\n",
    "        print(\"ANSWER: \", predicted[0])\n",
    "\n",
    "    return answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "reader = FiDReader()\n",
    "reader.model.load_state_dict(torch.load(TUNED_READER_PATH))\n",
    "#reader.load_model(TUNED_READER_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### E5 + FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading query E5-model...\n",
      "Loading document E5-model...\n",
      "Loading precomputed e5-base...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`embedding_function` is expected to be an Embeddings object, support for passing in a function will soon be removed.\n"
     ]
    }
   ],
   "source": [
    "TUNED_RETRIEVER_PATH = '/home/dzigen/Desktop/ITMO/ВКР/КМУ2024/logs/join_e5_fid_triviaqa/retriever_bestmodel.pt'\n",
    "\n",
    "e5_retriever = E5Retriever()\n",
    "e5_retriever.model.load_state_dict(torch.load(TUNED_RETRIEVER_PATH))\n",
    "#colb_retriever.load_model(TUNED_RETRIEVER_PATH)\n",
    "e5_retriever.load_base(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/9 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QUERY:  What is a RETRO approach\n",
      "CONTEXTS:\n",
      " Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study\n",
      "To answer the above question and bridge the missing gap, we perform an extensive study on RETRO, as to the best of our knowledge, RETRO is the only retrieval-augmented autoregressive LM that supports large-scale pretraining with retrieval on the massive pretraining corpus with hundreds of billion or trillion tokens.\n",
      "\n",
      "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study\n",
      "These results further substantiate the potential of RETRO, which is pre-trained with retrieval capabilities, as a promising approach.\n",
      "\n",
      "Improving language models by retrieving\n",
      "from trillions of tokens\n",
      "Retro models are ﬂexible and can be used without retrieval at evaluation and still achieve comparable performance to baseline models.\n",
      "\n",
      "Shall We Pretrain Autoregressive Language Models with Retrieval? A Comprehensive Study\n",
      "For open-domain QA tasks, RETRO achieves considerably superior performance than retrievalaugmented GPT that incorporates retrieval during fine-tuning across different model sizes and datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 1/9 [00:02<00:21,  2.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  Retrieval\n",
      "QUERY:  What is a kNN-LM approach\n",
      "CONTEXTS:\n",
      " GENERALIZATION THROUGH MEMORIZATION : NEAREST NEIGHBOR LANGUAGE MODELS\n",
      "We introduce kNN-LM, an approach that extends a pre-trained LM by linearly interpolating its next word distribution with a k-nearest neighbors (kNN) model.\n",
      "\n",
      "COPY IS ALL YOU NEED\n",
      "• kNN-LM (Khandelwal et al., 2020) is a retrieval-augmented generation model, which extends a pre-trained neural language model by linearly interpolating its next token distribution with a k-nearest neighbors (kNN) model.\n",
      "\n",
      "GENERALIZATION THROUGH MEMORIZATION : NEAREST NEIGHBOR LANGUAGE MODELS\n",
      "We introduce kNN-LMs, which extend a pre-trained neural language model (LM) by linearly interpolating it with a k-nearest neighbors (kNN) model.\n",
      "\n",
      "GENERALIZATION THROUGH MEMORIZATION : NEAREST NEIGHBOR LANGUAGE MODELS\n",
      "The kNN-LM involves augmenting such a pre-trained LM with a nearest neighbors retrieval mechanism, without any additional training (the representations learned by the LM remain unchanged).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 2/9 [00:03<00:10,  1.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  We introduce kNN-LMs\n",
      "QUERY:  What is a DPR approach\n",
      "CONTEXTS:\n",
      " Relevance-guided Supervision for OpenQA with ColBERT\n",
      "(2020) propose a dense passage retriever (DPR) that directly trains the architecture in Figure 2(a) for retrieval, relying on a simple approach to collect positives and negatives.\n",
      "\n",
      "Dense Passage Retrieval for Open-Domain Question Answering\n",
      "Given a collection of M text passages, the goal of our dense passage retriever (DPR) is to index all the passages in a low-dimensional and continuous space, such that it can retrieve efﬁciently the top k passages relevant to the input question for the reader at run-time.\n",
      "\n",
      "Relevance-guided Supervision for OpenQA with ColBERT\n",
      "Using this simple strategy, DPR considerably outperforms both ORQA and REALM and established a new state-of-the-art for extractive OpenQA.\n",
      "\n",
      "Dense Passage Retrieval for Open-Domain Question Answering\n",
      "While both methods include additional pretraining tasks and employ an expensive end-to-end training regime, DPR manages to outperform them on both NQ and TriviaQA, simply by focusing on learning a strong passage retrieval model using pairs of questions and answers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 3/9 [00:04<00:07,  1.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  Dense Passage Retrieval\n",
      "QUERY:  What is a RAG approach\n",
      "CONTEXTS:\n",
      " SELF-RAG : LEARNING TO RETRIEVE , GENERATE , AND CRITIQUE THROUGH SELF-REFLECTION\n",
      "Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues.\n",
      "\n",
      "Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "The RAG research paradigm is continuously evolving, and we categorize it into three stages: Naive RAG, Advanced RAG, and Modular RAG, as showed in Figure 3.\n",
      "\n",
      "Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "Retrieval-Augmented Generation (RAG) has emerged as a promising solution by incorporating knowledge from external databases.\n",
      "\n",
      "SELF-RAG : LEARNING TO RETRIEVE , GENERATE , AND CRITIQUE THROUGH SELF-REFLECTION\n",
      "A few concurrent works2 on RAG propose new training or prompting strategies to improve widely-adopted RAG approaches.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 4/9 [00:04<00:04,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  ad hoc approach\n",
      "QUERY:  What is a FiD approach\n",
      "CONTEXTS:\n",
      " End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering\n",
      "FiD-KD is a complex training procedure that requires multiple training stages and performs knowledge distillation with inter-attention scores.\n",
      "\n",
      "End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering\n",
      "The current best approach for training multi-document reader and retriever is FiD-KD (Izacard and Grave, 2021a).\n",
      "\n",
      "End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering\n",
      "First, the FiD reader is trained from the ﬁrst term of the EMDR2 objective in which its likelihood is conditioned on all the retrieved documents, similar to how the reader is used at test time.\n",
      "\n",
      "Improving language models by retrieving\n",
      "from trillions of tokens\n",
      "More recently, Emdr2 (Sachan et al., 2021) extends FiD by using an expectation-maximization algorithm to train the retriever end-to-end and achieves state of the art results compared to similarly sized models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 5/9 [00:05<00:04,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  Achieves State of the Art Results Compared to Similar sized Models\n",
      "QUERY:  What is a EMDR2 approach\n",
      "CONTEXTS:\n",
      " End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering\n",
      "EMDR2 is a framework that can be used to train retrieval-augmented text generation models for any task.\n",
      "\n",
      "End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering\n",
      "EMDR2 achieves new state-of-the-art results for models of comparable size on all datasets, outperforming recent approaches by 2-3 absolute exact match points.\n",
      "\n",
      "End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering\n",
      "We presented EMDR2, an end-to-end training method for retrievalaugmented question answering systems.\n",
      "\n",
      "End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering\n",
      "While EMDR2 has the potential to improve language models in the low-resource setting (as demonstrated by our results on WebQ in §3.4), it could exhibit typical biases that are associated with large language models.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 6/9 [00:07<00:03,  1.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  A Framework for End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering\n",
      "QUERY:  What is a Atlas approach\n",
      "CONTEXTS:\n",
      " Atlas: Few-shot Learning with\n",
      "Retrieval Augmented Language Models\n",
      "We also provided detailed ablations and analyses for what factors are important when training such retrieval-augmented models, and demonstrated Atlas’s updateability, interpretability and controlability capabilities.\n",
      "\n",
      "Atlas: Few-shot Learning with\n",
      "Retrieval Augmented Language Models\n",
      "We ﬁnally evaluate this model, called Atlas, on diﬀerent natural language understanding tasks in few-shot and full dataset settings.\n",
      "\n",
      "Atlas: Few-shot Learning with\n",
      "Retrieval Augmented Language Models\n",
      "In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples.\n",
      "\n",
      "Atlas: Few-shot Learning with\n",
      "Retrieval Augmented Language Models\n",
      "In the full dataset setting, Atlas is within 3% to the state-of-the-art for 3 datasets, and sets the state-of-the-art in the remaining ﬁve datasets.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 7/9 [00:08<00:02,  1.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  Atlas: Few-shot Learning with Retrieval Augmented Language Models\n",
      "QUERY:  What is a REPLUG approach\n",
      "CONTEXTS:\n",
      " REPLUG: Retrieval-Augmented Black-Box Language Models\n",
      "In all settings, REPLUG ˜ improve the performance of various black-box language models, showing the effectiveness and generality of our approach.\n",
      "\n",
      "REPLUG: Retrieval-Augmented Black-Box Language Models\n",
      "We introduce REPLUG, a retrieval-augmented language modeling paradigm that treats the language model as a black box and augments it with a tuneable retrieval model.\n",
      "\n",
      "REPLUG: Retrieval-Augmented Black-Box Language Models\n",
      "We introduce REPLUG, a retrieval-augmented language modeling framework that treats the language model (LM) as a black box and augments it with a tuneable retrieval model.\n",
      "\n",
      "Retrieval-Augmented Generation for Large Language Models: A Survey\n",
      "REPLUG [72] utilizes a retriever and an LLM to calculate the probability distributions of the retrieved documents and then performs supervised training by computing the KL divergence.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 8/9 [00:09<00:00,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  Retrieval-Augmented Black-Box Language Models\n",
      "QUERY:  What is a ColBERT approach\n",
      "CONTEXTS:\n",
      " ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\n",
      "ColBERT prescribes a simple framework for balancing the quality and cost of neural IR, particularly deep language models like BERT.\n",
      "\n",
      "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\n",
      "Recall that ColBERT can be used for re-ranking the output of another retrieval model, typically a term-based model, or directly for end-to-end retrieval from a document collection.\n",
      "\n",
      "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\n",
      "To reconcile efciency and contextualization in IR, we propose ColBERT, a ranking model based on contextualized late interaction over BERT.\n",
      "\n",
      "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT\n",
      "In this paper, we introduced ColBERT, a novel ranking model that employs contextualized late interaction over deep LMs (in particular, BERT) for efcient retrieval.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:09<00:00,  1.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER:  Efficient and Effective Passage Search\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Retrieval',\n",
       " 'We introduce kNN-LMs',\n",
       " 'Dense Passage Retrieval',\n",
       " 'ad hoc approach',\n",
       " 'Achieves State of the Art Results Compared to Similar sized Models',\n",
       " 'A Framework for End-to-End Training of Multi-Document Reader and Retriever for Open-Domain Question Answering',\n",
       " 'Atlas: Few-shot Learning with Retrieval Augmented Language Models',\n",
       " 'Retrieval-Augmented Black-Box Language Models',\n",
       " 'Efficient and Effective Passage Search']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inference(reader, e5_retriever, QUESTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25ColBERT + FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNED_RETRIEVER_PATH = ''\n",
    "\n",
    "colb_retriever = BM25ColBertRetriever()\n",
    "colb_retriever.load_model(TUNED_RETRIEVER_PATH)\n",
    "colb_retriever.load_base(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(reader, colb_retriever, QUESTIONS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BM25E5 + FID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TUNED_RETRIEVER_PATH = ''\n",
    "\n",
    "e5_retriever = BM25ColBertRetriever()\n",
    "e5_retriever.load_model(TUNED_RETRIEVER_PATH)\n",
    "e5_retriever.load_base(BASE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inference(reader, e5_retriever, QUESTIONS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
