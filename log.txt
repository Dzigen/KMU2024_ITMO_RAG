/home/ubuntu/.local/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/ubuntu/.local/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/ubuntu/.local/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading Reader-model...
Loading Tuned weights...
Load tuned FiD-model...
Loading metrics...
Loading Meteor...
Loading ExactMatch
Loadeing Retriever-model...
Loading base ColBERT-model...
Loading precomputed bm25-base...
Loading Tuned weights...
Load tuned ColBERT-model...
Loading metrics...
Prepare Datasets and Dataloaders...
5000 1000
==RUN_START==
Reader-model parameters count:  {'all': 247577856, 'trainable': 52993536}
Retriever-model parameters count:  {'all': 124694848, 'trainable': 21903424}
Init folder to save
Saving used nn-arch...
Saving grad info...
Saving used config...
Init train objectives
===LEARNING START===
Epoch 1 start:
  0%|          | 0/2500 [00:00<?, ?it/s]  0%|          | 0/2500 [00:08<?, ?it/s, avg_loss=5.44]  0%|          | 1/2500 [00:08<5:58:50,  8.62s/it, avg_loss=5.44]  0%|          | 1/2500 [00:13<5:58:50,  8.62s/it, avg_loss=5.41]  0%|          | 2/2500 [00:13<4:36:19,  6.64s/it, avg_loss=5.41]  0%|          | 2/2500 [00:19<4:36:19,  6.64s/it, avg_loss=5.35]  0%|          | 3/2500 [00:19<4:18:32,  6.21s/it, avg_loss=5.35]  0%|          | 3/2500 [00:24<4:18:32,  6.21s/it, avg_loss=5.11]  0%|          | 4/2500 [00:25<4:11:06,  6.04s/it, avg_loss=5.11]  0%|          | 4/2500 [00:30<4:11:06,  6.04s/it, avg_loss=5.02]  0%|          | 5/2500 [00:31<4:08:17,  5.97s/it, avg_loss=5.02]  0%|          | 5/2500 [00:36<4:08:17,  5.97s/it, avg_loss=4.53]  0%|          | 6/2500 [00:36<3:59:38,  5.77s/it, avg_loss=4.53]  0%|          | 6/2500 [00:43<3:59:38,  5.77s/it, avg_loss=4.42]  0%|          | 7/2500 [00:43<4:16:06,  6.16s/it, avg_loss=4.42]  0%|          | 7/2500 [00:48<4:16:06,  6.16s/it, avg_loss=3.55]  0%|          | 8/2500 [00:48<4:02:09,  5.83s/it, avg_loss=3.55]  0%|          | 8/2500 [00:53<4:02:09,  5.83s/it, avg_loss=3.68]  0%|          | 9/2500 [00:54<3:57:15,  5.71s/it, avg_loss=3.68]  0%|          | 9/2500 [00:59<3:57:15,  5.71s/it, avg_loss=3.87]  0%|          | 10/2500 [01:00<4:00:45,  5.80s/it, avg_loss=3.87]  0%|          | 10/2500 [01:05<4:00:45,  5.80s/it, avg_loss=4.04]  0%|          | 11/2500 [01:05<3:56:50,  5.71s/it, avg_loss=4.04]  0%|          | 11/2500 [01:10<3:56:50,  5.71s/it, avg_loss=4.14]  0%|          | 12/2500 [01:10<3:49:15,  5.53s/it, avg_loss=4.14]  0%|          | 12/2500 [01:16<3:49:15,  5.53s/it, avg_loss=4.24]  1%|          | 13/2500 [01:16<3:56:40,  5.71s/it, avg_loss=4.24]  1%|          | 13/2500 [01:22<3:56:40,  5.71s/it, avg_loss=4.08]  1%|          | 14/2500 [01:22<3:55:00,  5.67s/it, avg_loss=4.08]  1%|          | 14/2500 [01:28<3:55:00,  5.67s/it, avg_loss=4.03]  1%|          | 15/2500 [01:28<3:58:46,  5.77s/it, avg_loss=4.03]  1%|          | 15/2500 [01:34<3:58:46,  5.77s/it, avg_loss=4.1]   1%|          | 16/2500 [01:34<4:03:55,  5.89s/it, avg_loss=4.1]  1%|          | 16/2500 [01:40<4:03:55,  5.89s/it, avg_loss=3.93]  1%|          | 17/2500 [01:41<4:11:56,  6.09s/it, avg_loss=3.93]  1%|          | 17/2500 [01:46<4:11:56,  6.09s/it, avg_loss=4.01]  1%|          | 18/2500 [01:46<3:59:10,  5.78s/it, avg_loss=4.01]  1%|          | 18/2500 [01:51<3:59:10,  5.78s/it, avg_loss=3.86]  1%|          | 19/2500 [01:51<3:54:17,  5.67s/it, avg_loss=3.86]  1%|          | 19/2500 [01:57<3:54:17,  5.67s/it, avg_loss=3.9]   1%|          | 20/2500 [01:57<3:58:59,  5.78s/it, avg_loss=3.9]  1%|          | 20/2500 [02:02<3:58:59,  5.78s/it, avg_loss=3.87]  1%|          | 21/2500 [02:02<3:47:49,  5.51s/it, avg_loss=3.87]  1%|          | 21/2500 [02:07<3:47:49,  5.51s/it, avg_loss=3.85]  1%|          | 22/2500 [02:07<3:45:55,  5.47s/it, avg_loss=3.85]  1%|          | 22/2500 [02:14<3:45:55,  5.47s/it, avg_loss=3.98]  1%|          | 23/2500 [02:14<3:56:09,  5.72s/it, avg_loss=3.98]  1%|          | 23/2500 [02:19<3:56:09,  5.72s/it, avg_loss=4]     1%|          | 24/2500 [02:20<3:58:56,  5.79s/it, avg_loss=4]  1%|          | 24/2500 [02:26<3:58:56,  5.79s/it, avg_loss=4]  1%|          | 25/2500 [02:26<4:05:22,  5.95s/it, avg_loss=4]  1%|          | 25/2500 [02:32<4:05:22,  5.95s/it, avg_loss=4.05]  1%|          | 26/2500 [02:32<4:10:10,  6.07s/it, avg_loss=4.05]  1%|          | 26/2500 [02:37<4:10:10,  6.07s/it, avg_loss=3.98]  1%|          | 27/2500 [02:37<3:56:50,  5.75s/it, avg_loss=3.98]  1%|          | 27/2500 [02:44<3:56:50,  5.75s/it, avg_loss=3.95]  1%|          | 28/2500 [02:44<4:13:45,  6.16s/it, avg_loss=3.95]  1%|          | 28/2500 [02:50<4:13:45,  6.16s/it, avg_loss=3.84]  1%|          | 29/2500 [02:51<4:14:38,  6.18s/it, avg_loss=3.84]  1%|          | 29/2500 [02:56<4:14:38,  6.18s/it, avg_loss=3.8]   1%|          | 30/2500 [02:56<4:03:38,  5.92s/it, avg_loss=3.8]  1%|          | 30/2500 [03:01<4:03:38,  5.92s/it, avg_loss=3.78]  1%|          | 31/2500 [03:01<3:55:17,  5.72s/it, avg_loss=3.78]  1%|          | 31/2500 [03:06<3:55:17,  5.72s/it, avg_loss=3.73]  1%|▏         | 32/2500 [03:07<3:50:47,  5.61s/it, avg_loss=3.73]  1%|▏         | 32/2500 [03:13<3:50:47,  5.61s/it, avg_loss=3.74]  1%|▏         | 33/2500 [03:13<4:01:39,  5.88s/it, avg_loss=3.74]  1%|▏         | 33/2500 [03:18<4:01:39,  5.88s/it, avg_loss=3.74]  1%|▏         | 34/2500 [03:18<3:52:41,  5.66s/it, avg_loss=3.74]  1%|▏         | 34/2500 [03:23<3:52:41,  5.66s/it, avg_loss=3.63]  1%|▏         | 35/2500 [03:23<3:43:43,  5.45s/it, avg_loss=3.63]  1%|▏         | 35/2500 [03:29<3:43:43,  5.45s/it, avg_loss=3.53]  1%|▏         | 36/2500 [03:29<3:45:45,  5.50s/it, avg_loss=3.53]  1%|▏         | 36/2500 [03:34<3:45:45,  5.50s/it, avg_loss=3.56]  1%|▏         | 37/2500 [03:34<3:44:34,  5.47s/it, avg_loss=3.56]  1%|▏         | 37/2500 [03:39<3:44:34,  5.47s/it, avg_loss=3.54]  2%|▏         | 38/2500 [03:39<3:34:56,  5.24s/it, avg_loss=3.54]  2%|▏         | 38/2500 [03:44<3:34:56,  5.24s/it, avg_loss=3.54]  2%|▏         | 39/2500 [03:44<3:37:09,  5.29s/it, avg_loss=3.54]  2%|▏         | 39/2500 [03:50<3:37:09,  5.29s/it, avg_loss=3.55]  2%|▏         | 40/2500 [03:50<3:44:15,  5.47s/it, avg_loss=3.55]  2%|▏         | 40/2500 [03:55<3:44:15,  5.47s/it, avg_loss=3.56]  2%|▏         | 41/2500 [03:56<3:41:56,  5.42s/it, avg_loss=3.56]  2%|▏         | 41/2500 [04:01<3:41:56,  5.42s/it, avg_loss=3.6]   2%|▏         | 42/2500 [04:01<3:42:27,  5.43s/it, avg_loss=3.6]  2%|▏         | 42/2500 [04:06<3:42:27,  5.43s/it, avg_loss=3.55]  2%|▏         | 43/2500 [04:07<3:44:32,  5.48s/it, avg_loss=3.55]  2%|▏         | 43/2500 [04:12<3:44:32,  5.48s/it, avg_loss=3.57]  2%|▏         | 44/2500 [04:12<3:47:17,  5.55s/it, avg_loss=3.57]  2%|▏         | 44/2500 [04:18<3:47:17,  5.55s/it, avg_loss=3.51]  2%|▏         | 45/2500 [04:18<3:46:45,  5.54s/it, avg_loss=3.51]  2%|▏         | 45/2500 [04:23<3:46:45,  5.54s/it, avg_loss=3.43]  2%|▏         | 46/2500 [04:24<3:48:33,  5.59s/it, avg_loss=3.43]  2%|▏         | 46/2500 [04:29<3:48:33,  5.59s/it, avg_loss=3.45]  2%|▏         | 47/2500 [04:29<3:50:42,  5.64s/it, avg_loss=3.45]  2%|▏         | 47/2500 [04:34<3:50:42,  5.64s/it, avg_loss=3.5]   2%|▏         | 48/2500 [04:35<3:45:56,  5.53s/it, avg_loss=3.5]  2%|▏         | 48/2500 [04:41<3:45:56,  5.53s/it, avg_loss=3.59]  2%|▏         | 49/2500 [04:41<3:56:28,  5.79s/it, avg_loss=3.59]  2%|▏         | 49/2500 [04:46<3:56:28,  5.79s/it, avg_loss=3.55]  2%|▏         | 50/2500 [04:47<3:54:05,  5.73s/it, avg_loss=3.55]  2%|▏         | 50/2500 [04:51<3:54:05,  5.73s/it, avg_loss=3.46]  2%|▏         | 51/2500 [04:52<3:46:09,  5.54s/it, avg_loss=3.46]  2%|▏         | 51/2500 [04:57<3:46:09,  5.54s/it, avg_loss=3.4]   2%|▏         | 52/2500 [04:57<3:44:10,  5.49s/it, avg_loss=3.4]  2%|▏         | 52/2500 [05:02<3:44:10,  5.49s/it, avg_loss=3.34]  2%|▏         | 53/2500 [05:02<3:36:47,  5.32s/it, avg_loss=3.34]  2%|▏         | 53/2500 [05:07<3:36:47,  5.32s/it, avg_loss=3.35]  2%|▏         | 54/2500 [05:07<3:31:18,  5.18s/it, avg_loss=3.35]  2%|▏         | 54/2500 [05:12<3:31:18,  5.18s/it, avg_loss=3.34]  2%|▏         | 55/2500 [05:12<3:37:12,  5.33s/it, avg_loss=3.34]  2%|▏         | 55/2500 [05:18<3:37:12,  5.33s/it, avg_loss=3.34]  2%|▏         | 56/2500 [05:19<3:46:58,  5.57s/it, avg_loss=3.34]  2%|▏         | 56/2500 [05:24<3:46:58,  5.57s/it, avg_loss=3.29]  2%|▏         | 57/2500 [05:24<3:48:08,  5.60s/it, avg_loss=3.29]  2%|▏         | 57/2500 [05:29<3:48:08,  5.60s/it, avg_loss=3.33]  2%|▏         | 58/2500 [05:30<3:44:37,  5.52s/it, avg_loss=3.33]  2%|▏         | 58/2500 [05:35<3:44:37,  5.52s/it, avg_loss=3.28]  2%|▏         | 59/2500 [05:35<3:42:02,  5.46s/it, avg_loss=3.28]  2%|▏         | 59/2500 [05:40<3:42:02,  5.46s/it, avg_loss=3.32]  2%|▏         | 60/2500 [05:40<3:39:04,  5.39s/it, avg_loss=3.32]  2%|▏         | 60/2500 [05:45<3:39:04,  5.39s/it, avg_loss=3.29]  2%|▏         | 61/2500 [05:45<3:36:47,  5.33s/it, avg_loss=3.29]  2%|▏         | 61/2500 [05:51<3:36:47,  5.33s/it, avg_loss=3.31]  2%|▏         | 62/2500 [05:51<3:45:44,  5.56s/it, avg_loss=3.31]  2%|▏         | 62/2500 [05:57<3:45:44,  5.56s/it, avg_loss=3.34]  3%|▎         | 63/2500 [05:57<3:48:23,  5.62s/it, avg_loss=3.34]