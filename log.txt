/home/ubuntu/.local/lib/python3.11/site-packages/transformers/utils/generic.py:441: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/ubuntu/.local/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
/home/ubuntu/.local/lib/python3.11/site-packages/transformers/utils/generic.py:309: UserWarning: torch.utils._pytree._register_pytree_node is deprecated. Please use torch.utils._pytree.register_pytree_node instead.
  _torch_pytree._register_pytree_node(
You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']
You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.
Loading Reader-model...
Loading Tuned weights...
Load tuned FiD-model...
Loading metrics...
Loading Meteor...
Loading ExactMatch
Loadeing Retriever-model...
Loading base ColBERT-model...
Loading precomputed bm25-base...
Loading Tuned weights...
Load tuned ColBERT-model...
Loading metrics...
Prepare Datasets and Dataloaders...
5000 1000
==RUN_START==
Reader-model parameters count:  {'all': 247577856, 'trainable': 52993536}
Retriever-model parameters count:  {'all': 124694848, 'trainable': 21903424}
Init folder to save
Saving used nn-arch...
Saving grad info...
Saving used config...
Init train objectives
===LEARNING START===
Epoch 1 start:
  0%|          | 0/2500 [00:00<?, ?it/s]  0%|          | 0/2500 [00:11<?, ?it/s, avg_loss=5.44]  0%|          | 1/2500 [00:11<7:38:47, 11.02s/it, avg_loss=5.44]  0%|          | 1/2500 [00:18<7:38:47, 11.02s/it, avg_loss=5.41]  0%|          | 2/2500 [00:18<6:16:00,  9.03s/it, avg_loss=5.41]  0%|          | 2/2500 [00:26<6:16:00,  9.03s/it, avg_loss=5.35]  0%|          | 3/2500 [00:26<5:58:46,  8.62s/it, avg_loss=5.35]  0%|          | 3/2500 [00:34<5:58:46,  8.62s/it, avg_loss=5.11]  0%|          | 4/2500 [00:34<5:42:58,  8.24s/it, avg_loss=5.11]  0%|          | 4/2500 [00:42<5:42:58,  8.24s/it, avg_loss=5.01]  0%|          | 5/2500 [00:42<5:43:24,  8.26s/it, avg_loss=5.01]  0%|          | 5/2500 [00:50<5:43:24,  8.26s/it, avg_loss=4.52]  0%|          | 6/2500 [00:50<5:36:50,  8.10s/it, avg_loss=4.52]  0%|          | 6/2500 [00:59<5:36:50,  8.10s/it, avg_loss=4.41]  0%|          | 7/2500 [00:59<5:54:36,  8.53s/it, avg_loss=4.41]  0%|          | 7/2500 [01:07<5:54:36,  8.53s/it, avg_loss=3.58]  0%|          | 8/2500 [01:07<5:42:47,  8.25s/it, avg_loss=3.58]  0%|          | 8/2500 [01:15<5:42:47,  8.25s/it, avg_loss=3.71]  0%|          | 9/2500 [01:15<5:38:03,  8.14s/it, avg_loss=3.71]  0%|          | 9/2500 [01:23<5:38:03,  8.14s/it, avg_loss=3.9]   0%|          | 10/2500 [01:23<5:39:14,  8.17s/it, avg_loss=3.9]  0%|          | 10/2500 [01:31<5:39:14,  8.17s/it, avg_loss=4.06]  0%|          | 11/2500 [01:31<5:32:58,  8.03s/it, avg_loss=4.06]  0%|          | 11/2500 [01:39<5:32:58,  8.03s/it, avg_loss=4.16]  0%|          | 12/2500 [01:39<5:32:59,  8.03s/it, avg_loss=4.16]  0%|          | 12/2500 [01:47<5:32:59,  8.03s/it, avg_loss=4.25]  1%|          | 13/2500 [01:47<5:36:36,  8.12s/it, avg_loss=4.25]  1%|          | 13/2500 [01:55<5:36:36,  8.12s/it, avg_loss=4.08]  1%|          | 14/2500 [01:55<5:31:54,  8.01s/it, avg_loss=4.08]  1%|          | 14/2500 [02:04<5:31:54,  8.01s/it, avg_loss=4.04]  1%|          | 15/2500 [02:04<5:37:58,  8.16s/it, avg_loss=4.04]  1%|          | 15/2500 [02:13<5:37:58,  8.16s/it, avg_loss=4.12]  1%|          | 16/2500 [02:13<5:48:29,  8.42s/it, avg_loss=4.12]  1%|          | 16/2500 [02:22<5:48:29,  8.42s/it, avg_loss=3.93]  1%|          | 17/2500 [02:22<5:59:52,  8.70s/it, avg_loss=3.93]  1%|          | 17/2500 [02:29<5:59:52,  8.70s/it, avg_loss=4.01]  1%|          | 18/2500 [02:29<5:38:11,  8.18s/it, avg_loss=4.01]  1%|          | 18/2500 [02:37<5:38:11,  8.18s/it, avg_loss=3.84]  1%|          | 19/2500 [02:37<5:32:55,  8.05s/it, avg_loss=3.84]  1%|          | 19/2500 [02:45<5:32:55,  8.05s/it, avg_loss=3.89]  1%|          | 20/2500 [02:45<5:35:58,  8.13s/it, avg_loss=3.89]  1%|          | 20/2500 [02:52<5:35:58,  8.13s/it, avg_loss=3.85]  1%|          | 21/2500 [02:52<5:21:47,  7.79s/it, avg_loss=3.85]  1%|          | 21/2500 [02:59<5:21:47,  7.79s/it, avg_loss=3.82]  1%|          | 22/2500 [02:59<5:17:59,  7.70s/it, avg_loss=3.82]  1%|          | 22/2500 [03:08<5:17:59,  7.70s/it, avg_loss=3.99]  1%|          | 23/2500 [03:08<5:29:32,  7.98s/it, avg_loss=3.99]  1%|          | 23/2500 [03:16<5:29:32,  7.98s/it, avg_loss=4.02]  1%|          | 24/2500 [03:16<5:33:17,  8.08s/it, avg_loss=4.02]  1%|          | 24/2500 [03:25<5:33:17,  8.08s/it, avg_loss=4]     1%|          | 25/2500 [03:25<5:41:52,  8.29s/it, avg_loss=4]  1%|          | 25/2500 [03:34<5:41:52,  8.29s/it, avg_loss=4.04]  1%|          | 26/2500 [03:34<5:46:09,  8.39s/it, avg_loss=4.04]  1%|          | 26/2500 [03:41<5:46:09,  8.39s/it, avg_loss=3.99]  1%|          | 27/2500 [03:41<5:33:53,  8.10s/it, avg_loss=3.99]  1%|          | 27/2500 [03:51<5:33:53,  8.10s/it, avg_loss=3.97]  1%|          | 28/2500 [03:51<5:55:51,  8.64s/it, avg_loss=3.97]  1%|          | 28/2500 [04:01<5:55:51,  8.64s/it, avg_loss=3.86]  1%|          | 29/2500 [04:01<6:07:39,  8.93s/it, avg_loss=3.86]  1%|          | 29/2500 [04:08<6:07:39,  8.93s/it, avg_loss=3.83]  1%|          | 30/2500 [04:08<5:52:59,  8.57s/it, avg_loss=3.83]  1%|          | 30/2500 [04:16<5:52:59,  8.57s/it, avg_loss=3.81]  1%|          | 31/2500 [04:16<5:41:53,  8.31s/it, avg_loss=3.81]  1%|          | 31/2500 [04:24<5:41:53,  8.31s/it, avg_loss=3.76]  1%|▏         | 32/2500 [04:24<5:34:48,  8.14s/it, avg_loss=3.76]  1%|▏         | 32/2500 [04:32<5:34:48,  8.14s/it, avg_loss=3.76]  1%|▏         | 33/2500 [04:32<5:28:54,  8.00s/it, avg_loss=3.76]  1%|▏         | 33/2500 [04:39<5:28:54,  8.00s/it, avg_loss=3.76]  1%|▏         | 34/2500 [04:39<5:20:59,  7.81s/it, avg_loss=3.76]  1%|▏         | 34/2500 [04:46<5:20:59,  7.81s/it, avg_loss=3.65]  1%|▏         | 35/2500 [04:46<5:14:07,  7.65s/it, avg_loss=3.65]  1%|▏         | 35/2500 [04:54<5:14:07,  7.65s/it, avg_loss=3.53]  1%|▏         | 36/2500 [04:54<5:18:31,  7.76s/it, avg_loss=3.53]  1%|▏         | 36/2500 [05:02<5:18:31,  7.76s/it, avg_loss=3.57]  1%|▏         | 37/2500 [05:02<5:15:24,  7.68s/it, avg_loss=3.57]  1%|▏         | 37/2500 [05:09<5:15:24,  7.68s/it, avg_loss=3.56]  2%|▏         | 38/2500 [05:09<5:06:13,  7.46s/it, avg_loss=3.56]  2%|▏         | 38/2500 [05:16<5:06:13,  7.46s/it, avg_loss=3.57]  2%|▏         | 39/2500 [05:16<5:08:30,  7.52s/it, avg_loss=3.57]  2%|▏         | 39/2500 [05:25<5:08:30,  7.52s/it, avg_loss=3.57]  2%|▏         | 40/2500 [05:25<5:19:07,  7.78s/it, avg_loss=3.57]  2%|▏         | 40/2500 [05:32<5:19:07,  7.78s/it, avg_loss=3.59]  2%|▏         | 41/2500 [05:32<5:17:58,  7.76s/it, avg_loss=3.59]  2%|▏         | 41/2500 [05:40<5:17:58,  7.76s/it, avg_loss=3.63]  2%|▏         | 42/2500 [05:40<5:21:15,  7.84s/it, avg_loss=3.63]  2%|▏         | 42/2500 [05:49<5:21:15,  7.84s/it, avg_loss=3.58]  2%|▏         | 43/2500 [05:49<5:23:53,  7.91s/it, avg_loss=3.58]  2%|▏         | 43/2500 [05:57<5:23:53,  7.91s/it, avg_loss=3.58]  2%|▏         | 44/2500 [05:57<5:27:03,  7.99s/it, avg_loss=3.58]  2%|▏         | 44/2500 [06:05<5:27:03,  7.99s/it, avg_loss=3.47]  2%|▏         | 45/2500 [06:05<5:25:18,  7.95s/it, avg_loss=3.47]  2%|▏         | 45/2500 [06:12<5:25:18,  7.95s/it, avg_loss=3.43]  2%|▏         | 46/2500 [06:12<5:23:02,  7.90s/it, avg_loss=3.43]  2%|▏         | 46/2500 [06:21<5:23:02,  7.90s/it, avg_loss=3.46]  2%|▏         | 47/2500 [06:21<5:25:59,  7.97s/it, avg_loss=3.46]  2%|▏         | 47/2500 [06:28<5:25:59,  7.97s/it, avg_loss=3.5]   2%|▏         | 48/2500 [06:28<5:23:35,  7.92s/it, avg_loss=3.5]  2%|▏         | 48/2500 [06:37<5:23:35,  7.92s/it, avg_loss=3.59]  2%|▏         | 49/2500 [06:37<5:33:39,  8.17s/it, avg_loss=3.59]  2%|▏         | 49/2500 [06:45<5:33:39,  8.17s/it, avg_loss=3.57]  2%|▏         | 50/2500 [06:45<5:31:39,  8.12s/it, avg_loss=3.57]  2%|▏         | 50/2500 [06:53<5:31:39,  8.12s/it, avg_loss=3.48]  2%|▏         | 51/2500 [06:53<5:24:49,  7.96s/it, avg_loss=3.48]  2%|▏         | 51/2500 [07:00<5:24:49,  7.96s/it, avg_loss=3.42]  2%|▏         | 52/2500 [07:00<5:22:28,  7.90s/it, avg_loss=3.42]  2%|▏         | 52/2500 [07:08<5:22:28,  7.90s/it, avg_loss=3.36]  2%|▏         | 53/2500 [07:08<5:14:21,  7.71s/it, avg_loss=3.36]  2%|▏         | 53/2500 [07:15<5:14:21,  7.71s/it, avg_loss=3.36]  2%|▏         | 54/2500 [07:15<5:10:02,  7.61s/it, avg_loss=3.36]  2%|▏         | 54/2500 [07:23<5:10:02,  7.61s/it, avg_loss=3.36]  2%|▏         | 55/2500 [07:23<5:17:38,  7.79s/it, avg_loss=3.36]  2%|▏         | 55/2500 [07:32<5:17:38,  7.79s/it, avg_loss=3.37]  2%|▏         | 56/2500 [07:32<5:28:15,  8.06s/it, avg_loss=3.37]  2%|▏         | 56/2500 [07:40<5:28:15,  8.06s/it, avg_loss=3.3]   2%|▏         | 57/2500 [07:40<5:30:41,  8.12s/it, avg_loss=3.3]  2%|▏         | 57/2500 [07:48<5:30:41,  8.12s/it, avg_loss=3.34]  2%|▏         | 58/2500 [07:48<5:28:05,  8.06s/it, avg_loss=3.34]  2%|▏         | 58/2500 [07:56<5:28:05,  8.06s/it, avg_loss=3.28]  2%|▏         | 59/2500 [07:56<5:26:50,  8.03s/it, avg_loss=3.28]  2%|▏         | 59/2500 [08:04<5:26:50,  8.03s/it, avg_loss=3.29]  2%|▏         | 60/2500 [08:04<5:23:27,  7.95s/it, avg_loss=3.29]  2%|▏         | 60/2500 [08:12<5:23:27,  7.95s/it, avg_loss=3.25]  2%|▏         | 61/2500 [08:12<5:19:17,  7.85s/it, avg_loss=3.25]  2%|▏         | 61/2500 [08:20<5:19:17,  7.85s/it, avg_loss=3.26]  2%|▏         | 62/2500 [08:20<5:29:17,  8.10s/it, avg_loss=3.26]  2%|▏         | 62/2500 [08:29<5:29:17,  8.10s/it, avg_loss=3.28]  3%|▎         | 63/2500 [08:29<5:33:27,  8.21s/it, avg_loss=3.28]  3%|▎         | 63/2500 [08:38<5:33:27,  8.21s/it, avg_loss=3.25]  3%|▎         | 64/2500 [08:38<5:44:13,  8.48s/it, avg_loss=3.25]  3%|▎         | 64/2500 [08:45<5:44:13,  8.48s/it, avg_loss=3.23]  3%|▎         | 65/2500 [08:45<5:31:24,  8.17s/it, avg_loss=3.23]  3%|▎         | 65/2500 [08:54<5:31:24,  8.17s/it, avg_loss=3.23]  3%|▎         | 66/2500 [08:54<5:33:16,  8.22s/it, avg_loss=3.23]  3%|▎         | 66/2500 [09:01<5:33:16,  8.22s/it, avg_loss=3.16]  3%|▎         | 67/2500 [09:01<5:26:26,  8.05s/it, avg_loss=3.16]  3%|▎         | 67/2500 [09:10<5:26:26,  8.05s/it, avg_loss=3.21]  3%|▎         | 68/2500 [09:10<5:32:11,  8.20s/it, avg_loss=3.21]  3%|▎         | 68/2500 [09:18<5:32:11,  8.20s/it, avg_loss=3.17]  3%|▎         | 69/2500 [09:18<5:31:49,  8.19s/it, avg_loss=3.17]  3%|▎         | 69/2500 [09:26<5:31:49,  8.19s/it, avg_loss=3.16]  3%|▎         | 70/2500 [09:26<5:32:49,  8.22s/it, avg_loss=3.16]  3%|▎         | 70/2500 [09:34<5:32:49,  8.22s/it, avg_loss=3.18]  3%|▎         | 71/2500 [09:34<5:24:32,  8.02s/it, avg_loss=3.18]