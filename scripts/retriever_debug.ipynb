{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers import BM25Retriever, EnsembleRetriever\n",
    "from langchain_core.documents import Document\n",
    "import pickle\n",
    "import torch\n",
    "from transformers import T5Tokenizer\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import sys \n",
    "sys.path.insert(0, '/home/dzigen/Desktop/ITMO/ВКР/КМУ2024')\n",
    "\n",
    "from src.colbert_model import ColBERT, ColBertTokenizer\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e5_model = AutoModel.from_pretrained('intfloat/e5-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e5_tokenizer = AutoTokenizer.from_pretrained('intfloat/e5-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each input text should start with \"query: \" or \"passage: \".\n",
    "# For tasks other than retrieval, you can simply use the \"query: \" prefix.\n",
    "input_texts = ['query: how much protein should a female eat',\n",
    "               'query: summit define',\n",
    "               \"passage: As a general guideline, the CDC's average requirement of protein for women ages 19 to 70 is 46 grams per day. But, as you can see from this chart, you'll need to increase that if you're expecting or training for a marathon. Check out the chart below to see how much protein you should be eating each day.\",\n",
    "               \"passage: Definition of summit for English Language Learners. : 1  the highest point of a mountain : the top of a mountain. : 2  the highest level. : 3  a meeting or series of meetings between the leaders of two or more governments.\"]\n",
    "\n",
    "# Tokenize the input texts\n",
    "batch_dict = e5_tokenizer(input_texts, max_length=512, padding=True, truncation=True, return_tensors='pt')\n",
    "\n",
    "outputs = e5_model(**batch_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_pool(last_hidden_states, attention_mask):\n",
    "    last_hidden = last_hidden_states.masked_fill(~attention_mask[..., None].bool(), 0.0)\n",
    "    print(last_hidden.shape)\n",
    "    return last_hidden.sum(dim=1) / attention_mask.sum(dim=1)[..., None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 75, 768])\n",
      "[[89.99481201171875, 67.25457763671875], [68.84741973876953, 91.38423156738281]]\n"
     ]
    }
   ],
   "source": [
    "embeddings = average_pool(outputs.last_hidden_state, batch_dict['attention_mask'])\n",
    "\n",
    "# normalize embeddings\n",
    "embeddings = F.normalize(embeddings, p=2, dim=1)\n",
    "scores = (embeddings[:2] @ embeddings[2:].T) * 100\n",
    "print(scores.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 768])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings[:2].unsqueeze(1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8999, 0.6725],\n",
       "        [0.6885, 0.9138]], grad_fn=<SumBackward1>)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(embeddings[:2].unsqueeze(1), embeddings[2:], dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# сделать E5 retriever\n",
    "# протестить bm25colbert retriever\n",
    "# протестить e5 retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class E5Retriever:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BM25ColBertRetriever:\n",
    "    def __init__(self, bm25_candidates=3, colbert_candidates=2, colbert_reddim=64, docs_bs=4) -> None:\n",
    "        self.bm25_cands = bm25_candidates\n",
    "        self.colbert_cands = colbert_candidates\n",
    "        self.docs_bs = docs_bs\n",
    "\n",
    "        self.colbert_model = ColBERT(candidates=bm25_candidates, reduced_dim=colbert_reddim)\n",
    "        self.colbert_tokenizer = ColBertTokenizer\n",
    "        self.tokenize = lambda x: self.colbert_tokenizer(\n",
    "            x, max_length=512, truncation=True, \n",
    "            padding=True, return_tensors='pt')\n",
    "\n",
    "    #\n",
    "    def load_bm25_base(self, pickle_file):\n",
    "        with open(pickle_file, 'rb') as bm25result_file:\n",
    "            self.bm25_model = pickle.load(bm25result_file)\n",
    "\n",
    "    #\n",
    "    def load_colbert_model(self, weights_path):\n",
    "        self.colbert_model.load_state_dict(torch.load(weights_path))\n",
    "\n",
    "    #\n",
    "    def texts2documents(self, texts):\n",
    "        return [Document(page_content=txt, metadata={'tokenized': self.tokenize(txt)}) \n",
    "                for txt in texts]\n",
    "\n",
    "    #\n",
    "    def make_bm25_base(self, texts, save_pickle_file=None):\n",
    "        documents = self.texts2documents(texts)\n",
    "        self.bm25_model = BM25Retriever.from_documents(documents, \n",
    "                                                     k=self.bm25_cands)\n",
    "\n",
    "        if save_pickle_file is not None:\n",
    "            with open(save_pickle_file, 'wb') as bm25result_file:\n",
    "                pickle.dump(self.bm25_model, bm25result_file)\n",
    "\n",
    "    #\n",
    "    def search(self, query, tokenized_query=None):\n",
    "        bm25_docs, tokenized_docs = self.bm25_retrieve(query)\n",
    "\n",
    "        if tokenized_query is None:\n",
    "            tokenized_query = self.tokenize(query)\n",
    "        colbert_docs, scores = self.colbert_retrieve(\n",
    "            tokenized_query, bm25_docs, tokenized_docs)\n",
    "\n",
    "        return colbert_docs, scores\n",
    "\n",
    "    #\n",
    "    def bm25_retrieve(self, query):\n",
    "        relevant_documents = self.bm25_model.get_relevant_documents(query)\n",
    "        text_docs = np.array([doc.page_content for doc in relevant_documents])\n",
    "        tokenized_docs = [doc.metadata['tokenized'] for doc in relevant_documents]\n",
    "        \n",
    "        docs_dataset = DocDataset(tokenized_docs)\n",
    "        docs_laoder = DataLoader(docs_dataset, batch_size=self.docs_bs, \n",
    "                                 collate_fn=custom_collate, shuffle=False)\n",
    "\n",
    "        return text_docs, docs_laoder\n",
    "\n",
    "    #\n",
    "    def colbert_retrieve(self, tokenized_query, bm25_docs, docs_loader):\n",
    "        all_scores = torch.tensor([], requires_grad=True)\n",
    "        for doc_batch in docs_loader:\n",
    "            print(\"doc_batch - \", doc_batch['input_ids'].shape)\n",
    "\n",
    "            scores = self.colbert_model(tokenized_query['input_ids'], tokenized_query['attention_mask'],\n",
    "                                    doc_batch['input_ids'], doc_batch['attention_mask'])\n",
    "            all_scores = torch.cat((all_scores, scores),dim=1)\n",
    "\n",
    "        flat_scores = all_scores.view(-1)\n",
    "        _, indices = torch.sort(flat_scores, descending=True)\n",
    "        relevant_ids = indices[:self.colbert_cands]\n",
    "\n",
    "        print(bm25_docs)\n",
    "        print(flat_scores)\n",
    "\n",
    "        return bm25_docs[relevant_ids], flat_scores.take(relevant_ids)\n",
    "    \n",
    "class DocDataset(Dataset):\n",
    "    def __init__(self, docs):\n",
    "        self._data = docs\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self._data[idx]\n",
    "    \n",
    "    def __getitems__(self, idxs):\n",
    "        return [self.__getitem__(idx) for idx in idxs]\n",
    "        \n",
    "\n",
    "def custom_collate(data):\n",
    "\n",
    "    input_ids = torch.cat([item['input_ids'] for item in data], 0)\n",
    "    attention_mask = torch.cat([item['attention_mask'] for item in data], 0)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": input_ids, \n",
    "        \"attention_mask\": attention_mask\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del retriever\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at FacebookAI/roberta-base and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "retriever = BM25ColBertRetriever(colbert_candidates=5, bm25_candidates=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.make_bm25_base([\"France\", \"Russia\", \"United Kingdom\", \"Japan\", \"USA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['United Kingdom', 'France', 'USA', 'Russia', 'Japan'], dtype='<U14'),\n",
       " tensor([4.0000, 3.6716, 3.6680, 3.6404, 3.6176], grad_fn=<TakeBackward0>))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del res\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc_batch -  torch.Size([4, 512])\n",
      "=='colbert foward'-func\n",
      "q_ids -  torch.Size([1, 512])\n",
      "q_masks -  torch.Size([1, 512])\n",
      "d_ids -  torch.Size([4, 512])\n",
      "d_masks -  torch.Size([4, 512])\n",
      "\n",
      "=='q_enc'-function: \n",
      "q_ids -  torch.Size([1, 512])\n",
      "q_masks -  torch.Size([1, 512])\n",
      "\n",
      "out:\n",
      "encoder - torch.Size([1, 512, 768])\n",
      "dim reduce - torch.Size([1, 512, 64])\n",
      "ecoded queries =  torch.Size([1, 512, 64])\n",
      "=='d_enc'-function: \n",
      "d_ids -  torch.Size([4, 512])\n",
      "d_masks -  torch.Size([4, 512])\n",
      "\n",
      "out: \n",
      "encoder -  torch.Size([4, 512, 768])\n",
      "dim reduce -  torch.Size([4, 512, 64])\n",
      "encoded documents =  torch.Size([4, 512, 64])\n",
      "=='compute_score'-function: \n",
      "q_hidden -  torch.Size([1, 512, 64])\n",
      "q_mask -  torch.Size([1, 512])\n",
      "d_hidden -  torch.Size([4, 512, 64])\n",
      "d_mask -  torch.Size([4, 512])\n",
      "\n",
      "cos similarity - torch.Size([4, 512, 512])\n",
      "mask doc tokens -  torch.Size([4, 512, 512])\n",
      "find max sim -  torch.Size([4, 512])\n",
      "mask query tokens -  torch.Size([4, 512])\n",
      "sum scores -  torch.Size([4])\n",
      "out scores =  torch.Size([1, 4])\n",
      "doc_batch -  torch.Size([1, 512])\n",
      "=='colbert foward'-func\n",
      "q_ids -  torch.Size([1, 512])\n",
      "q_masks -  torch.Size([1, 512])\n",
      "d_ids -  torch.Size([1, 512])\n",
      "d_masks -  torch.Size([1, 512])\n",
      "\n",
      "=='q_enc'-function: \n",
      "q_ids -  torch.Size([1, 512])\n",
      "q_masks -  torch.Size([1, 512])\n",
      "\n",
      "out:\n",
      "encoder - torch.Size([1, 512, 768])\n",
      "dim reduce - torch.Size([1, 512, 64])\n",
      "ecoded queries =  torch.Size([1, 512, 64])\n",
      "=='d_enc'-function: \n",
      "d_ids -  torch.Size([1, 512])\n",
      "d_masks -  torch.Size([1, 512])\n",
      "\n",
      "out: \n",
      "encoder -  torch.Size([1, 512, 768])\n",
      "dim reduce -  torch.Size([1, 512, 64])\n",
      "encoded documents =  torch.Size([1, 512, 64])\n",
      "=='compute_score'-function: \n",
      "q_hidden -  torch.Size([1, 512, 64])\n",
      "q_mask -  torch.Size([1, 512])\n",
      "d_hidden -  torch.Size([1, 512, 64])\n",
      "d_mask -  torch.Size([1, 512])\n",
      "\n",
      "cos similarity - torch.Size([1, 512, 512])\n",
      "mask doc tokens -  torch.Size([1, 512, 512])\n",
      "find max sim -  torch.Size([1, 512])\n",
      "mask query tokens -  torch.Size([1, 512])\n",
      "sum scores -  torch.Size([1])\n",
      "out scores =  torch.Size([1, 1])\n",
      "['United Kingdom' 'USA' 'Japan' 'Russia' 'France']\n",
      "tensor([4.0000, 3.6680, 3.6176, 3.6404, 3.6716], grad_fn=<ViewBackward0>)\n"
     ]
    }
   ],
   "source": [
    "res = retriever.search(\"United Kingdom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res[1].sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.colbert_model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever.colbert_model.query_encoder.encoder.layer[11].output.dense.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
