{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import gc\n",
    "import json\n",
    "from time import time\n",
    "import os\n",
    "import albumentations as A\n",
    "import cv2\n",
    "from dataclasses import dataclass\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import sys\n",
    "sys.path.insert(0, \"/home/dzigen/Desktop/ITMO/ВКР/КМУ2024/\")\n",
    "\n",
    "from src.readers.fid import FiDReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzigen/miniconda3/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5.py:246: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "reader = FiDReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.randint(0, 255, size=(2,2,512))\n",
    "attention_mask = torch.randint(0, 255, size=(2,2,512))\n",
    "labels = torch.randint(0, 255, size=(2,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = reader.model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.tokenizer.batch_decode(out, skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 10])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward 2\n",
      "init dimension:  torch.Size([2, 2, 512])\n",
      "2d resize:  torch.Size([2, 1024])\n",
      "forward 3\n",
      "2d resize:  torch.Size([2, 1024])\n",
      "---\n",
      "candidates flat:  torch.Size([4, 512])\n",
      "encoder output  torch.Size([4, 512, 768])\n",
      "candidates concatenation:\n",
      "last_hidden_state torch.Size([2, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "out = reader.model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Reader Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only Retriever Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0436)\n",
      "tensor(1.0436)\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "output_score = torch.eye(6) #torch.randn(6,6)\n",
    "targets = torch.arange(0,6)\n",
    "\n",
    "auto_loss = criterion(output_score, targets)\n",
    "print(auto_loss)\n",
    "\n",
    "manual_loss = torch.mean(-torch.log(F.softmax(output_score, dim=1).gather(1, targets.view(-1,1)) ))\n",
    "print(manual_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0261)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader_topk_loss = 0.2\n",
    "reader_k_loss = torch.tensor([[0.9,0.1,0.5]])\n",
    "retriever_k_scores = torch.tensor([[5,1,3]])\n",
    "criterion(reader_topk_loss, reader_k_loss, retriever_k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.6946)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader_topk_loss = 0.8\n",
    "reader_k_loss = torch.tensor([[0.9,0.8,1]])\n",
    "retriever_k_scores = torch.tensor([[5,3,3]])\n",
    "criterion(reader_topk_loss, reader_k_loss, retriever_k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([[1,2,3],[4,5,6]]).argmax(dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reader + Frozen Retriever Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.randint(0,255, size=(1,2,512))\n",
    "attention_mask = torch.randint(0,2, size=(1,2,512))\n",
    "\n",
    "labels = torch.randint(0,255, size=(1,512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [204, 21820, 296, 1], 'attention_mask': [1, 1, 1, 1]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reader.tokenizer(\" 2 hello world\", add_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward 2\n",
      "init dimension:  torch.Size([1, 2, 512])\n",
      "2d resize:  torch.Size([1, 1024])\n",
      "forward 3\n",
      "2d resize:  torch.Size([1, 1024])\n",
      "---\n",
      "candidates flat:  torch.Size([2, 512])\n",
      "encoder output  torch.Size([2, 512, 768])\n",
      "candidates concatenation:\n",
      "last_hidden_state torch.Size([1, 1024, 768])\n"
     ]
    }
   ],
   "source": [
    "output = reader.model(input_ids=input_ids,attention_mask=attention_mask, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512, 32128])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 512])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2608e-06, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(output.logits, dim=-1)[0][0][111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2608e-06, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(output.logits.view(512, 32128), dim=-1).gather(1,labels)[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(15.1423, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "outs = torch.mean(-torch.log(F.softmax(output.logits, dim=-1).gather(2,labels.view(1, 512, -1))).view(1, 512), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([15.1423], grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.2608e-06, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(output.logits, dim=-1)[0][0][111]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dzigen/miniconda3/lib/python3.11/site-packages/transformers/models/t5/tokenization_t5.py:246: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-base automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "reader = FiDReader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_reshape = output[:,:-1].contiguous().view(-1, output.shape[-1])\n",
    "                trg_batch = trg_batch[:, 1:].contiguous().view(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joint Reader + Retriever Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def join_evaluate(reader, retriever, loader, criterion,\n",
    "                  reader_metrics, retriever_metrics, config):\n",
    "    reader.model.eval()\n",
    "    retriever.model.eval()\n",
    "    scores = {\n",
    "        'reader': {\n",
    "            'bleu': [],'rouge': [],\n",
    "            'meteor': [],'em': []\n",
    "        },\n",
    "        'retriever': {\n",
    "            'MRR': [],\n",
    "            'mAP': []\n",
    "        }\n",
    "    }\n",
    "\n",
    "    losses = []\n",
    "    process = tqdm(loader)\n",
    "    for batch in process:\n",
    "        gc.collect()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "        q_ids = batch['q_ids'].to(config.device, non_blocking=True)\n",
    "        q_masks = batch['q_mask'].to(config.device, non_blocking=True)\n",
    "\n",
    "        for i in range(len(batch['q_text'])):\n",
    "            # retriever-part\n",
    "            texts, scores, metadata = retriever.search(batch['q_text'][i], \n",
    "                                             {'input_ids': q_ids[i].view(1,-1),\n",
    "                                              'attention_mask':q_masks[i].view(1,-1)})\n",
    "\n",
    "            if 'relevant_d_ids' in batch.keys():\n",
    "                predicted_d_ids = [meta['in_base_index'] for meta in metadata]\n",
    "\n",
    "                scores['retriever']['MRR'].append(retriever_metrics.mAP(\n",
    "                    predicted_d_ids,batch['relevant_d_ids'][i]))\n",
    "                scores['retriever']['mAP'].append(retriever_metrics.MRR(\n",
    "                    predicted_d_ids,batch['relevant_d_ids'][i]))\n",
    "\n",
    "            # reader-part\n",
    "            prep_txts = list(map(lambda t: \"QUESTION: \"+t+\" PASSAGE: \"+batch['q_text'][i], texts))\n",
    "            tokenized_txts = reader.tokenize(prep_txts)\n",
    "\n",
    "            output = reader.model(\n",
    "                input_ids=tokenized_txts['input_ids'].view(1, len(prep_txts), -1), \n",
    "                labels=batch['label'][i].view(1,-1), \n",
    "                attention_mask=tokenized_txts['attention_mask'].view(1, len(prep_txts), -1))\n",
    "            \n",
    "            reader_topk_loss = output.loss.item()\n",
    "\n",
    "            output = reader.model(\n",
    "                input_ids=tokenized_txts['input_ids'].view(len(prep_txts), 1, -1), \n",
    "                labels=batch['label'][i].view(1,-1), \n",
    "                attention_mask=tokenized_txts['attention_mask'].view(len(prep_txts), 1, -1))\n",
    "\n",
    "            reader_k_loss = ...\n",
    "\n",
    "            loss = criterion(reader_topk_loss, reader_k_loss, scores)\n",
    "\n",
    "            losses.append(loss.item())\n",
    "            process.set_postfix({\"avg_loss\": np.mean(losses)})\n",
    "\n",
    "            output = reader.model.generate(\n",
    "                input_ids=tokenized_txts['input_ids'],\n",
    "                attention_mask=tokenized_txts['attention_mask'], max_length=128)   \n",
    "\n",
    "            predicted = reader.tokenizer.batch_decode(output, skip_special_tokens=True)\n",
    "\n",
    "            scores['reader']['bleu'].append(reader_metrics.bleu(predicted,[batch['label_text'][i]]))\n",
    "            scores['reader']['rouge'].append(reader_metrics.rouge(predicted,[batch['label_text'][i]]))\n",
    "            scores['reader']['meteor'].append(reader_metrics.meteor(predicted,[batch['label_text'][i]]))\n",
    "            scores['reader']['em'].append(reader_metrics.exact_match(predicted,[batch['label_text'][i]]))\n",
    "\n",
    "def join_train(reader, retriever, train_loader, optimizer, criterion,\n",
    "                config):\n",
    "    pass\n",
    "\n",
    "\n",
    "    # извлекаем k релевантных пассажей по query\n",
    "\n",
    "    # заного прогоняем пассажи через retriever-часть\n",
    "    # получаем скоры\n",
    "\n",
    "    # конкатенируем пассажи и запросы\n",
    "    # передаём в read модель\n",
    "    # получем общий loss\n",
    "\n",
    "    # подаём пассаж для каждлого запроса по отдельности\n",
    "    # получем loss для каждого пассажа в отдельности\n",
    "    # делаем stop gradioents\n",
    "\n",
    "    # комбинируем скор из ретривера, общий лосс от ридера и разделённый лосс от ридера"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
